---
title: "Ch12: Model tuning"
output:
  html_notebook: 
    toc: true
    toc_float: true
    theme: cerulean
---

## Introduction

For many types of prediction model (e.g., K-nearest neighbors), hyperparameters that define how the model behaves can have substantial influence on the predictions and mode fit, but cannot be estimated from the data.  This chapter focuses on the process of tuning, or finding optimal values for the hyperparameters.  However, one area where you do not use tuning to select an optimal choice for hyperparameter is in choosing a prior-- this is meant to represent your belief and uncertainty about the data.

```{r notebook setup}
# Libraries and data
library(pacman)
p_load(tidyverse, tidymodels, broom, splines, janitor, patchwork, 
       tidyposterior, rstanarm)

data(ames, package = "modeldata")
ames <- ames %>% 
  clean_names() %>% 
  mutate(sale_price = log10(sale_price))

# Split and read workflows
set.seed(123)
ames_split <- initial_split(ames, prop = 0.8, strata = sale_price)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)

lm_workflow <- readRDS("../data/ames_lm_workflow.Rds")
lm_fit <- readRDS("../data/ames_lm_fit.Rds")
ames_recipe <- readRDS("../data/ames_recipe.Rds")
```

## What do we optimize?

First, we'll use a two class problem and use the log likelihood as the tuning parameter.  The textbook uses the `two_class_dat` data set from the `modeldata` package.

```{r log likelihood}
# Load data
data(two_class_dat, package = "modeldata")

# Split
two_class_split <- initial_split(two_class_dat, strata = Class)
training_set <- training(two_class_split)

# Define function to fit a GLM and extract the log likelihood
log_likelihood <- function(...) {
  out <- logistic_reg() %>% 
    set_engine("glm", ...) %>% 
    fit(Class ~ ., data = training_set) %>% 
    glance() %>% 
    select(logLik)
  
  return(out)
}

# Visualize where the boundary may be
ggplot(two_class_dat, aes(A, B, color = Class)) + 
  geom_point() 

# Fit models and compute log likelihood
log_likelihood() %>% 
  bind_rows(
    log_likelihood(family = binomial(link = "probit")), 
    log_likelihood(family = binomial(link = "cloglog"))
  ) %>% 
  mutate(link = c("logit", "probit", "c-log-log")) %>% 
  arrange(desc(logLik))

```

Now we can use resampling to evaluate the improvement in the statistics.

```{r resample likelihoods}
set.seed(1292)
(rs <- vfold_cv(training_set, repeats = 10))

# Return performance metrics
likelihood_loss <- function(...) {
  
  perf_meas <- metric_set(roc_auc, mn_log_loss) # mean log loss
  
  out <- logistic_reg() %>% 
    set_engine("glm", ...) %>% 
    fit_resamples(Class ~ A + B, resamples = rs, metrics = perf_meas) %>% 
    collect_metrics(summarize = FALSE) %>% 
    select(id, id2, .metric, .estimate)
  
  return(out)
}

# Get results 
resampled_res <- bind_rows(
  likelihood_loss() %>% mutate(model = "logistic"), 
  likelihood_loss(family = binomial(link = "probit")) %>% mutate(model = "probit"), 
  likelihood_loss(family = binomial(link = "cloglog")) %>% mutate(model = "c-log-log")
)

# Convert log loss to log-likelihood
resampled_summary <- resampled_res %>% 
  mutate(.estimate = ifelse(.metric == "mn_log_loss", -.estimate, .estimate)) %>% 
  group_by(model, .metric) %>% 
  summarize(
    mean = mean(.estimate, na.rm = TRUE),
    std_err = sd(.estimate, na.rm = TRUE),
    .groups = "drop"
  )

# Plot
resampled_summary %>% 
  ggplot(aes(mean, model)) + 
  geom_point() + 
  geom_errorbar(aes(xmin = mean - 1.96 * std_err, 
                    xmax = mean + 1.96 * std_err), 
                width = 0.1) + 
  facet_wrap(~ .metric, scales = "free", ncol = 1)
```

## Consequences of poor parameter estimates

Although hyperparameters can be fine-tuned to improve predictions, it can also lead to over-fitting where the model pays too much attention to the training data and is unable to generalize well to new data.  To illustrate, the authors use a neural network trained on the same data and show that with increasing complexity, the model essentially memorizes the training data but doesn't generalize well to new data.

## General strategies for optimization 

Two general approaches are used for parameter optimization-- 1) grid search, which entails trying a pre-defined set of parameter values; and 2) iterative search, which sequentially modifies parameter values to try based on the previous results.  

## Tuning in tidymodels

In parsnip model objects, tuning parameters are either main arguments to the model specification function (e.g., the number of trees in one of a few random forest models) or engine specific, meaning they are arguments applied only to the specific underlying software used in the modeling.  Overall, tuning hyperparameters can quickly lead to models that are overly complex and have poor predictive performance when presented with new data.  Thus, the resampling approaches used previoulsy are needed for evaluation.    